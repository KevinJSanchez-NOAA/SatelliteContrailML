# Detect aircraft contrails with a Unet
## Goal
Label contrails pixel-by-pixel (semantic segmentation) from satellite images for the purpose of validating forecasting models (Models that forecast where contrails can form). The resulting labeled images had two classes (binary), 1. contrail, 2. not a contrail (or everything else).

## Summary of project
This project was mainly meant for me to experiment with applying a CNN to a large dataset. As the title states I aimed to automate the detection of contrails from satellite images with a Unet. Before this project I had taken Andrew Ngs online machine learning specialization course and implemented a Unet, but it was all spoon fed to me, the student, and fairly quick and easy. Upon implementing a Unet on my contrail dataset, I quickly ran into problems that were not covered in the class. Here I list some of the challenges that I had to overcome for this project:

1. Creating/Finding Labeled data for training
I began by creating a tool to manually label a training/test set (details found in x repository). Manually labeling the data was extremely tedious even with the point and click tool. And I found many cases where I was unsure if a cloud was a contrail or not. Eventually I abbandonded using my own labeled dataset because a dataset labeled by a collaborative group at Google and MIT became publically available (https://www.climatechange.ai/papers/icml2021/2). The group had multiple people label each image and had used "advected aircraft trajectories" to show where contrails could be (if they formed) based on aircraft flight paths. This improved the labelers confidence in identifying if a cloud was a contrail or simply a linear like cloud.

2. First implementation of Unet
My contrail dataset was several hundred GBs in size which obviously could not all fit into memory. In addition unlike standard RGB images, I had 6 channels, each representing a different wavelength instead of a different color. In testing I did not always use all 6 channels. Some made more sense to use for detecting contrails than others. I preprocessed my dataset (make my image mask from arrays of contrail polygon points determined by labelers, sliced single-images into several smaller reasonably sized images, standardized each channel) converted the dataset to a .tfrecords format and saved (yes I had another version of the same data... but I wont have to preprocess each time I train... or so I thought... more on that in the dataset imbalance section). I then had to set up the traning model to randomly load a batch of data that will fit into memory, then reallocate that memory to load another batch (TF memory leaks problems were a pain here...). 

3. Image size and blank space in my images
Due to the nature of satellite images, each pixel does not represent the same amount of area (pixels further from zenith cover more area). Thankfully the dataset was already had a Lambert Azimuthal Equal-Area transformation applied. With this transformation however, the images were no longer rectangular. The pixel values were still saved in a matrix but there was a long of 'blank' area around the image. First I wanted to remove as much of the blank space as possible so I applied a minimum-bounding rectangle to each image. This made each image a difference size, but that was fine since I was slicing them into smaller consistantly sized images. I experimented with making this blank area a third class and simply including it in the 'not a contrail' class, in the end I found the best solution was to make another channel where the image was all 1s and the 'blank' area was all 0s. This way the model could learn what part of the image to look for contrails from that channel and not need to also learn that from the original image channels.

4. Dataset imbalance
The dataset is highly unbalanced as only 0-~2% of pixels often represented contrails. Initially my model was 99% accurate!!... and simply predicted there was never a contrail... Anyway I first changed my evaluation metric to IOU and experimented with versions of loss functions that reduced the relative loss for well classified classes (the background or not contrail class). I also performed some image augmentation methods to shift the balance (removing some images with no contrails, duplicating+rotating/flipping images with contrails... etc. This added several more hyperparameters to test and resulted in several different preprocessed versions of the data.

## Results
Here is an example satellite image (11, 12 micron difference image) and corresponding preddiction result.
![result](result.PNG)

## What I could try next
It quickly became clear to me when I started this project that detecting contrails with machine learning would be very difficult because human observers struggle to confidently identify contrails. If we the human cannot do it well, then the model will surely also have a hard time. Something that I overlooked is that the human labelers had more infromation than I was giving the model. As I mentioned in the "Creating/Finding Labeled data for training" section, the labelers had "advected aircraft trajectories" to show where contrails could be (if they formed) based on aircraft flight paths to improve there confidence in when labeling clouds contrails. I believe the model would certainly benefit from having this information in the form of another channel. This will mean we cannot detect contrails in (near) real-time since it requires reanalysis windspeed data, but that is not necessary for our purpose of validating forecasting models. 
In addition to this, I could experiment with more complex CNN architectures (other than a standard Unet), but it is unclear to me that this will create significant improvement. Another approach that would require a new dataset would be to use a time series of images (from a geostationary satellite) to track contrails as they changed from easily identifiable lines to less defined lines. I will attempt to label such a dataset again by myself...

## What I should do better
I have preprocessing and training of the model completely separate (2 scripts). I do not think preprocessing in parrellel with training is reasonable in this case as preprocessing takes a few hours (longer than an epoch) and would drastically lengthen the training processes. Therefore, it is useful to keep the two processes sperate since preprocessing would only need to be performed once (unless I want to change hyperparameters in the preprocessing (i.e., how I balance the data). I'm sure there are ways to improve the efficiency of the preprocessing, in which case it may be fine to preprocess in parrallel with training. I should atleast create a batch file to run the preprocessing code (if necessary) then train the model.


