{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ea7090-2a21-46f5-ae21-e3e50b2f77f2",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## Import Packages and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb437a0-ae1e-4c58-83f8-4f78626be60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('cpu_compiler', '/home/builder/ktietz/aggregate/tensorflow_recipes/ci_cpu/tensorflow-base_1614583966145/_build_env/bin/x86_64-conda_cos6-linux-gnu-gcc'), ('cuda_compute_capabilities', ['compute_35', 'compute_52', 'compute_60', 'compute_61', 'compute_70', 'compute_75']), ('cuda_version', '10.1'), ('cudnn_version', '7'), ('is_cuda_build', True), ('is_rocm_build', False)])\n",
      "cuda version:  10.1\n",
      "cudNN version:  7\n",
      "TF version:  2.4.1\n"
     ]
    }
   ],
   "source": [
    "### -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May 24 13:26:13 2021\n",
    "\n",
    "@author: kjsanche\n",
    "\n",
    "Description: \n",
    "A function to load the 5 minute granules from MODIS channel 1 \n",
    "(0.65 microns) and the contrail mask for ML with a CNN.\n",
    "\n",
    "To do:\n",
    "\n",
    "-add transfer learning\n",
    "-add image augmentation\n",
    "-organize/markdown/comment code\n",
    "\n",
    "\n",
    "Input:\n",
    "Path   (string)\n",
    "\n",
    "        \n",
    "        \n",
    "Output:\n",
    "MODISCh1 (2D uint32)\n",
    "MASK     (2D uint16)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import struct\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "#from format_input import *\n",
    "from UNET_Functions_TL import unet_model, summary\n",
    "from Sat_contrail_read import Extract_RawDef, extract_img, extract_mask, extract_imglist, get_model_memory_usage\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('/home/kjsanche/Desktop/Projects/loss')\n",
    "from loss_function import *\n",
    "from tensorflow.python.ops.metrics_impl import false_positives, false_negatives\n",
    "import tensorflow.keras.metrics as tfm\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose,AveragePooling2D, MaxPooling2D, UpSampling2D, LeakyReLU, concatenate, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#config = tf.config.experimental\n",
    "#config.gpu_options.allow_growth = True\n",
    "#sess = tf.Session(config=config)\n",
    "\n",
    "sys_details = tf.sysconfig.get_build_info()\n",
    "print(sys_details)\n",
    "cudnn_version = sys_details[\"cudnn_version\"]\n",
    "cuda_version = sys_details[\"cuda_version\"]\n",
    "\n",
    "print('cuda version: ', cuda_version)\n",
    "print('cudNN version: ',cudnn_version)\n",
    "print('TF version: ', tf.version.VERSION)\n",
    "\n",
    "\n",
    "TFrecord_path ='/home/kjsanche/Desktop/ExternalSSD/SatContrailData/TFrecords/'\n",
    "Models_path ='/home/kjsanche/Desktop/ExternalSSD/SatContrailData/Models/'\n",
    "VGG16_weight = \"/home/kjsanche/Desktop/ExternalSSD/SatContrailData/pretrained_models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "\n",
    "\n",
    "filenames=tf.io.gfile.glob([TFrecord_path + '*v4*.tfrecords'])\n",
    "VALIDATION_SPLIT = .05\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 100\n",
    "AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
    "IMG_W=512\n",
    "IMG_H=1024\n",
    "ALPHA=BETA=0.5\n",
    "GAMMA=2\n",
    "N_CHANNELS = 3\n",
    "LEARNING_RATE = 0.003\n",
    "\n",
    "\n",
    "\n",
    "random.shuffle(filenames)\n",
    "split = int(len(filenames) * VALIDATION_SPLIT)\n",
    "\n",
    "training_filenames = filenames[split:]\n",
    "validation_filenames = filenames[:split]\n",
    "\n",
    "validation_steps = len(validation_filenames) // BATCH_SIZE\n",
    "steps_per_epoch = len(training_filenames)  // BATCH_SIZE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73030cfe-a7cf-4833-bf27-d5fe9926be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfr_element(element):\n",
    "    #use the same structure as above; it's kinda an outline of the structure we now want to create\n",
    "    data = {\n",
    "      'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'width':tf.io.FixedLenFeature([], tf.int64),\n",
    "      'depth':tf.io.FixedLenFeature([], tf.int64),\n",
    "      'raw_label':tf.io.FixedLenFeature([], tf.string),#tf.string = bytestring (not text string)\n",
    "      'raw_image' : tf.io.FixedLenFeature([], tf.string),#tf.string = bytestring (not text string)\n",
    "    }\n",
    "\n",
    "\n",
    "    content = tf.io.parse_single_example(element, data)\n",
    "\n",
    "    height = content['height']\n",
    "    #height=1024\n",
    "    width = content['width']\n",
    "    #width=1024*2\n",
    "    depth = content['depth']\n",
    "    #depth=7\n",
    "    raw_label = content['raw_label']\n",
    "    raw_image = content['raw_image']\n",
    "\n",
    "\n",
    "    #get our 'feature'-- our image -- and reshape it appropriately\n",
    "    feature = tf.io.parse_tensor(raw_image, out_type=tf.float16)\n",
    "\n",
    "    feature = tf.reshape(feature, shape=[height,width,depth])\n",
    "    label = tf.io.parse_tensor(raw_label, out_type=tf.int8)\n",
    "    label = tf.reshape(label, shape=[height,width])\n",
    "    return (feature, label)\n",
    "\n",
    "def get_batched_dataset(filenames):\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.experimental_deterministic = False\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.with_options(option_no_order)\n",
    "    dataset = dataset.interleave(tf.data.TFRecordDataset, cycle_length=16, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(parse_tfr_element, num_parallel_calls=AUTO)\n",
    "\n",
    "    dataset = dataset.cache() # This dataset fits in RAM\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) \n",
    "    dataset = dataset.prefetch(AUTO) #\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_training_dataset(training_filenames):\n",
    "    return get_batched_dataset(training_filenames)\n",
    "\n",
    "def get_validation_dataset(training_filenames):\n",
    "    return get_batched_dataset(validation_filenames)\n",
    "\n",
    "def get_dataset_large(tfr_dir:str=\"/home/kjsanche/Desktop/Projects/Sat_Contrail_Unet/Unet/content/\", pattern:str=\"*large_images.tfrecords\"):\n",
    "    files = glob.glob(tfr_dir+pattern, recursive=False)\n",
    "\n",
    "    #create the dataset\n",
    "    dataset = tf.data.TFRecordDataset(files)\n",
    "\n",
    "    #pass every single feature through our mapping function\n",
    "    dataset = dataset.map(parse_tfr_element)\n",
    "\n",
    "    return dataset\n",
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        #print(i)\n",
    "        #print(display_list[i].shape)\n",
    "        if i == 0:\n",
    "            plt.imshow(np.float32(display_list[i][:,:,7]))#-display_list[i][:,:,1]))\n",
    "        else:\n",
    "            plt.imshow(np.float32(1*display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95823e8e-cea2-4843-a8a1-88d23a1950da",
   "metadata": {},
   "source": [
    "The below code cell uses a lot of memory and therefore should only be used for testing and not be used during training."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f05f075-6b1b-4801-8e8d-e84c6e363970",
   "metadata": {},
   "source": [
    "testdataset = get_dataset_large(tfr_dir = TFrecord_path, pattern = '*.tfrecords')\n",
    "for image, mask in testdataset.take(3):\n",
    "    sample_image, sample_mask = image, mask\n",
    "    display([sample_image, sample_mask])\n",
    "print('mem usage: ', tf.config.experimental.get_memory_usage(\"GPU:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e431f70-397c-4ca4-8f8b-6461838d0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FocalTverskyLoss(targets, inputs, alpha=ALPHA, beta=BETA, gamma=GAMMA, smooth=1e-6):\n",
    "        '''\n",
    "        ... in the case of α=β=0.5 the Tversky index simplifies to be \n",
    "        the same as the Dice coefficient, which is also equal to the F1 \n",
    "        score. With α=β=1, Equation 2 produces Tanimoto coefficient, and \n",
    "        setting α+β=1 produces the set of Fβ scores. Larger βs weigh \n",
    "        recall higher than precision (by placing more emphasis on false negatives).\n",
    "        '''\n",
    "        targets = tf.cast(targets,tf.float32)\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = K.flatten(inputs)\n",
    "        targets = K.flatten(targets)\n",
    "        \n",
    "        #True Positives, False Positives & False Negatives\n",
    "        TP = K.sum((inputs * targets))\n",
    "        FP = K.sum(((1-targets) * inputs))\n",
    "        FN = K.sum((targets * (1-inputs)))\n",
    "               \n",
    "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
    "        FocalTversky = K.pow((1 - Tversky), gamma)\n",
    "        \n",
    "        return FocalTversky"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00a3168b-b2b3-4595-b179-70d77dcfa298",
   "metadata": {
    "tags": []
   },
   "source": [
    "class FalsePositives(tfm.FalsePositives):\n",
    "    def __init__(self, from_logits=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._from_logits = from_logits\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        if self._from_logits:\n",
    "            super(FalsePositives, self).update_state(y_true, tf.nn.sigmoid(y_pred), sample_weight)\n",
    "        else:\n",
    "            super(FalsePositives, self).update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82fed353-9c63-4bdd-8c05-ed6598b4c74d",
   "metadata": {},
   "source": [
    "    VGG16 = vgg16.VGG16(include_top=False, weights=VGG16_weight, input_shape=(512,1024,3))\n",
    "    #######################################################\n",
    "    inputs = Input(input_shape)\n",
    "    conv = Conv2D(32, # Number of filters\n",
    "                  3,   # Kernel size   \n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal')(inputs)\n",
    "    conv=tf.keras.layers.LeakyReLU(alpha=0.3)(conv)\n",
    "    conv = Conv2D(3, # Number of filters\n",
    "                  3,   # Kernel size\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal')(conv)\n",
    "    conv=tf.keras.layers.LeakyReLU(alpha=0.3)(conv)\n",
    "    next_layer = MaxPooling2D(pool_size = 2)(conv)\n",
    "    \n",
    "\n",
    "    last_layer = VGG16(next_layer)\n",
    "    )\n",
    "layers = [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d94077e-2d75-4c65-8af9-a1288f8abdd9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "VGG16 = vgg16.VGG16(include_top=False, weights=VGG16_weight, input_shape=(IMG_W, IMG_H, 3))\n",
    "print('Model loaded.')\n",
    "\n",
    "output_ = VGG16.output\n",
    "\n",
    "set_trainable = False\n",
    "for layer in VGG16.layers:\n",
    "    #if layer.name in ['block1_conv1']:\n",
    "    #    set_trainable = True\n",
    "    #if layer.name in ['block1_pool','block2_pool','block3_pool','block4_pool','block5_pool']:\n",
    "    layer.trainable = False\n",
    "\n",
    "vgg_model = Model(VGG16.input, output_)\n",
    "layers = [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ff7471a-9d55-4ad5-8042-6dcf23ad1e8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc9d87c-4d8e-46f6-9290-7ffeecf37e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem usage:  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('mem usage: ', tf.config.experimental.get_memory_usage(\"GPU:0\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9110de16-020f-4858-af0f-5a353d5e9046",
   "metadata": {},
   "source": [
    "K.clear_session()\n",
    "VGG16 = vgg16.VGG16(include_top=False, weights=VGG16_weight, input_shape=(IMG_W, IMG_H, 3))\n",
    "\n",
    "test = VGG16.layers[2:]\n",
    "print(VGG16.input)\n",
    "print(' ')\n",
    "print(VGG16.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4753335b-b129-4871-9f6c-e30013faf426",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa6672d-07ea-4ca9-b670-a1be2d82c5ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "def ModelEnhancer(VGG16_weight, input_shape=(96, 128, 3), n_classes=8):\n",
    "    \n",
    "    VGG16 = vgg16.VGG16(include_top=False, weights=VGG16_weight, input_shape=input_shape)\n",
    "    #######################################################\n",
    "    '''inputs = Input(input_shape)\n",
    "    conv = Conv2D(32, # Number of filters\n",
    "                  3,   # Kernel size   \n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal')(inputs)\n",
    "    conv=tf.keras.layers.LeakyReLU(alpha=0.3)(conv)\n",
    "    conv = Conv2D(3, # Number of filters\n",
    "                  3,   # Kernel size\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal')(conv)\n",
    "    VGG16input=tf.keras.layers.LeakyReLU(alpha=0.3)(conv)\n",
    "    \n",
    "    \n",
    "\n",
    "    last_layer = VGG16(VGG16input)\n",
    "    VGG16.output'''\n",
    "    ##################################################\n",
    "    \n",
    "\n",
    "    last_layer = VGG16.output\n",
    "    \n",
    "    set_trainable = True #False\n",
    "    for layer in VGG16.layers:\n",
    "        if layer.name in ['block1_conv1']:\n",
    "            set_trainable = True # because input is not actualy a 3 channel image\n",
    "        #if layer.name in ['block1_pool','block2_pool','block3_pool','block4_pool','block5_pool']:\n",
    "        layer.trainable = True\n",
    "            \n",
    "    \n",
    "    model_ = Conv2DTranspose(512,3,strides=(2, 2),padding='same')(last_layer)\n",
    "    model_ = LeakyReLU(0.1)(model_)\n",
    "    model_ = BatchNormalization()(model_)    \n",
    "    \n",
    "    concat_1 = concatenate([model_,VGG16.get_layer(\"block5_conv3\").output])\n",
    "    \n",
    "    model_ = Conv2D(512,(3,3),strides=(1, 1),padding='same')(concat_1)\n",
    "    model_ = LeakyReLU(0.1)(model_)\n",
    "    model_ = BatchNormalization()(model_)\n",
    "    \n",
    "    model_ = Conv2DTranspose(512,3,strides=(2, 2),padding='same')(model_)\n",
    "    \n",
    "    #model_ = Conv2DTranspose(512,3,strides=(2, 2),padding='same')(VGG16.get_layer(\"block4_pool\").output)\n",
    "    model_ = LeakyReLU(0.1)(model_)\n",
    "    model_ = BatchNormalization()(model_) \n",
    "    \n",
    "    concat_2 = concatenate([model_,VGG16.get_layer(\"block4_conv3\").output])\n",
    "    \n",
    "    model_ = Conv2D(512,(3,3),strides=(1, 1),padding='same')(concat_2)\n",
    "    model_ = LeakyReLU(0.1)(model_)\n",
    "    model_ = BatchNormalization()(model_)\n",
    "    \n",
    "    model_ = Conv2DTranspose(256,(3,3),strides=(2, 2),padding='same')(model_)\n",
    "    model_ = LeakyReLU(0.1)(model_)\n",
    "    model_ = BatchNormalization()(model_) \n",
    "    \n",
    "    concat_3 = concatenate([model_,VGG16.get_layer(\"block3_conv3\").output])\n",
    "    \n",
    "    model_ = Conv2D(256,(3,3),strides=(1, 1),padding='same')(concat_3)\n",
    "    model_ = LeakyReLU(0.1)(model_)\n",
    "    model_ = BatchNormalization()(model_)\n",
    "    \n",
    "    model_ = Conv2DTranspose(128,(3,3),strides=(2, 2),padding='same')(model_)\n",
    "    model_ = LeakyReLU(0.1)(model_)\n",
    "    model_ = BatchNormalization()(model_) \n",
    "    \n",
    "    concat_4 = concatenate([model_,VGG16.get_layer(\"block2_conv2\").output])\n",
    "    \n",
    "    model_ = Conv2D(128,(3,3),strides=(1, 1),padding='same')(concat_4)\n",
    "    model_ = LeakyReLU(0.1)(model_)\n",
    "    model_ = BatchNormalization()(model_)\n",
    "    \n",
    "    model_ = Conv2DTranspose(64,(3,3),strides=(2, 2),padding='same')(model_)\n",
    "    model_ = LeakyReLU(0.1)(model_)\n",
    "    model_ = BatchNormalization()(model_) \n",
    "    \n",
    "    concat_5 = concatenate([model_,VGG16.get_layer(\"block1_conv2\").output])\n",
    "    \n",
    "    model_ = Conv2D(64,(3,3),strides=(1, 1),padding='same')(concat_5)\n",
    "    model_ = LeakyReLU(0.1)(model_)\n",
    "    model_ = BatchNormalization()(model_)\n",
    "    \n",
    "    model_ = Conv2D(1,(3,3),strides=(1, 1),padding='same')(model_)\n",
    "    model_ = LeakyReLU(0.1)(model_)\n",
    "    model_ = BatchNormalization()(model_)\n",
    "    \n",
    "    model_ = Model(VGG16.inputs,tf.keras.activations.sigmoid(tf.squeeze(model_,3)))\n",
    "    \n",
    "    return model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d158a868-eee7-4ead-a31e-399a439eca29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________________________________________\n",
      "Layer (type)                              Output Shape                 Param #         Connected to                               \n",
      "==================================================================================================================================\n",
      "input_1 (InputLayer)                      [(None, 512, 1024, 3)]       0                                                          \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)                     (None, 512, 1024, 64)        1792            input_1[0][0]                              \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)                     (None, 512, 1024, 64)        36928           block1_conv1[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)                (None, 256, 512, 64)         0               block1_conv2[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)                     (None, 256, 512, 128)        73856           block1_pool[0][0]                          \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)                     (None, 256, 512, 128)        147584          block2_conv1[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)                (None, 128, 256, 128)        0               block2_conv2[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)                     (None, 128, 256, 256)        295168          block2_pool[0][0]                          \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)                     (None, 128, 256, 256)        590080          block3_conv1[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)                     (None, 128, 256, 256)        590080          block3_conv2[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)                (None, 64, 128, 256)         0               block3_conv3[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)                     (None, 64, 128, 512)         1180160         block3_pool[0][0]                          \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)                     (None, 64, 128, 512)         2359808         block4_conv1[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)                     (None, 64, 128, 512)         2359808         block4_conv2[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)                (None, 32, 64, 512)          0               block4_conv3[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)                     (None, 32, 64, 512)          2359808         block4_pool[0][0]                          \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)                     (None, 32, 64, 512)          2359808         block5_conv1[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)                     (None, 32, 64, 512)          2359808         block5_conv2[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)                (None, 16, 32, 512)          0               block5_conv3[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspose)        (None, 32, 64, 512)          2359808         block5_pool[0][0]                          \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)                   (None, 32, 64, 512)          0               conv2d_transpose[0][0]                     \n",
      "__________________________________________________________________________________________________________________________________\n",
      "batch_normalization (BatchNormalization)  (None, 32, 64, 512)          2048            leaky_re_lu[0][0]                          \n",
      "__________________________________________________________________________________________________________________________________\n",
      "concatenate (Concatenate)                 (None, 32, 64, 1024)         0               batch_normalization[0][0]                  \n",
      "                                                                                       block5_conv3[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                           (None, 32, 64, 512)          4719104         concatenate[0][0]                          \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)                 (None, 32, 64, 512)          0               conv2d[0][0]                               \n",
      "__________________________________________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNormalization (None, 32, 64, 512)          2048            leaky_re_lu_1[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTranspose)      (None, 64, 128, 512)         2359808         batch_normalization_1[0][0]                \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)                 (None, 64, 128, 512)         0               conv2d_transpose_1[0][0]                   \n",
      "__________________________________________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNormalization (None, 64, 128, 512)         2048            leaky_re_lu_2[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)               (None, 64, 128, 1024)        0               batch_normalization_2[0][0]                \n",
      "                                                                                       block4_conv3[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                         (None, 64, 128, 512)         4719104         concatenate_1[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)                 (None, 64, 128, 512)         0               conv2d_1[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNormalization (None, 64, 128, 512)         2048            leaky_re_lu_3[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTranspose)      (None, 128, 256, 256)        1179904         batch_normalization_3[0][0]                \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)                 (None, 128, 256, 256)        0               conv2d_transpose_2[0][0]                   \n",
      "__________________________________________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNormalization (None, 128, 256, 256)        1024            leaky_re_lu_4[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)               (None, 128, 256, 512)        0               batch_normalization_4[0][0]                \n",
      "                                                                                       block3_conv3[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                         (None, 128, 256, 256)        1179904         concatenate_2[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)                 (None, 128, 256, 256)        0               conv2d_2[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNormalization (None, 128, 256, 256)        1024            leaky_re_lu_5[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTranspose)      (None, 256, 512, 128)        295040          batch_normalization_5[0][0]                \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)                 (None, 256, 512, 128)        0               conv2d_transpose_3[0][0]                   \n",
      "__________________________________________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNormalization (None, 256, 512, 128)        512             leaky_re_lu_6[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)               (None, 256, 512, 256)        0               batch_normalization_6[0][0]                \n",
      "                                                                                       block2_conv2[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                         (None, 256, 512, 128)        295040          concatenate_3[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)                 (None, 256, 512, 128)        0               conv2d_3[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNormalization (None, 256, 512, 128)        512             leaky_re_lu_7[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTranspose)      (None, 512, 1024, 64)        73792           batch_normalization_7[0][0]                \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)                 (None, 512, 1024, 64)        0               conv2d_transpose_4[0][0]                   \n",
      "__________________________________________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNormalization (None, 512, 1024, 64)        256             leaky_re_lu_8[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)               (None, 512, 1024, 128)       0               batch_normalization_8[0][0]                \n",
      "                                                                                       block1_conv2[0][0]                         \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                         (None, 512, 1024, 64)        73792           concatenate_4[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)                 (None, 512, 1024, 64)        0               conv2d_4[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNormalization (None, 512, 1024, 64)        256             leaky_re_lu_9[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                         (None, 512, 1024, 1)         577             batch_normalization_9[0][0]                \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)                (None, 512, 1024, 1)         0               conv2d_5[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNormalizatio (None, 512, 1024, 1)         4               leaky_re_lu_10[0][0]                       \n",
      "__________________________________________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze (TFOpLambda)         (None, 512, 1024)            0               batch_normalization_10[0][0]               \n",
      "__________________________________________________________________________________________________________________________________\n",
      "tf.math.sigmoid (TFOpLambda)              (None, 512, 1024)            0               tf.compat.v1.squeeze[0][0]                 \n",
      "==================================================================================================================================\n",
      "Total params: 31,982,341\n",
      "Trainable params: 31,976,451\n",
      "Non-trainable params: 5,890\n",
      "__________________________________________________________________________________________________________________________________\n",
      "mem usage:  130569984\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#gamma>0 reduces the relative loss for well-classified examples \n",
    "#alpha is a weighted term whose value is α for positive(foreground) alpha = 1 does nothing. alpha = 0.25 is best\n",
    "#class and 1-α for negative(background) class.\n",
    "\n",
    "unet = ModelEnhancer(VGG16_weight,(IMG_W, IMG_H, N_CHANNELS),n_classes=1)\n",
    "#loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#loss=tfa.losses.SigmoidFocalCrossEntropy(),\n",
    "#loss=[BinaryFocalLoss(gamma=2,from_logits=True)],\n",
    "\n",
    "#Larger βs weigh recall higher than precision (by placing more emphasis on false negatives)\n",
    "#loss=FocalTverskyLoss(targets, inputs, alpha=0.5, beta=0.5, gamma=1, smooth=1e-6)\n",
    "\n",
    "unet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "              loss=[FocalTverskyLoss],\n",
    "              metrics=[tfm.Precision(), tfm.Recall(), tfm.FalseNegatives(), tfm.FalsePositives(), tfm.TruePositives(), tfm.TrueNegatives(), 'binary_accuracy'])\n",
    "\n",
    "unet.summary(line_length = 130)\n",
    "print('mem usage: ', tf.config.experimental.get_memory_usage(\"GPU:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b76f90-e07b-4920-917c-2f4e1dbe8a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.628\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7243e9e0e856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-69f02fd7e91d>\u001b[0m in \u001b[0;36mget_training_dataset\u001b[0;34m(training_filenames)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_training_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_batched_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_validation_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-69f02fd7e91d>\u001b[0m in \u001b[0;36mget_batched_dataset\u001b[0;34m(filenames)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moption_no_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_deterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moption_no_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAUTO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mlist_files\u001b[0;34m(file_pattern, shuffle, seed)\u001b[0m\n\u001b[1;32m   1222\u001b[0m           string_ops.reduce_join(file_pattern, separator=\", \"), name=\"message\")\n\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       assert_not_empty = control_flow_ops.Assert(\n\u001b[0m\u001b[1;32m   1225\u001b[0m           condition, [message], summarize=1, name=\"assert_not_empty\")\n\u001b[1;32m   1226\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massert_not_empty\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/tf_should_use.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;34m\"\"\"Decorates the input function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m       return _add_should_use_warning(fn(*args, **kwargs),\n\u001b[0m\u001b[1;32m    248\u001b[0m                                      \u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                                      error_in_function=error_in_function)\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mAssert\u001b[0;34m(condition, data, summarize, name)\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_n_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0mdata_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_summarize_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m       raise errors.InvalidArgumentError(\n\u001b[0m\u001b[1;32m    155\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: '"
     ]
    }
   ],
   "source": [
    "print(get_model_memory_usage(BATCH_SIZE, unet))\n",
    "\n",
    "\n",
    "training_data = get_training_dataset(training_filenames)\n",
    "validation_data = get_training_dataset(validation_filenames)\n",
    "print(training_data)\n",
    "model_history = unet.fit(training_data, validation_data=validation_data, validation_steps=validation_steps, steps_per_epoch=steps_per_epoch, epochs=EPOCHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1123b2-3cac-4932-93a0-12064c9e8de1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## unet.save_weights(Models_path+'model')\n",
    "#unet = tf.keras.models.load_model(Models_path+'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dbf810-7398-46ca-914a-1c3537bb5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(15,15))\n",
    "\n",
    "axs[0,0].plot(model_history.history['binary_accuracy'])\n",
    "axs[0,0].plot(model_history.history['val_binary_accuracy'])\n",
    "axs[0,0].set_title('model accuracy')\n",
    "axs[0,0].set_ylabel('accuracy')\n",
    "\n",
    "#axs[0].legend(['train', 'val'], loc='upper left')\n",
    "#axs[0].show()\n",
    "\n",
    "axs[1,0].plot(model_history.history['loss'])\n",
    "axs[1,0].plot(model_history.history['val_loss'])\n",
    "axs[1,0].set_title('model loss')\n",
    "axs[1,0].set_ylabel('loss')\n",
    "\n",
    "axs[1,0].legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "\n",
    "\n",
    "F1 = 2*np.divide(np.multiply(model_history.history['precision'],model_history.history['recall']), np.add(model_history.history['precision'], model_history.history['recall']))\n",
    "F1_val = 2*np.divide(np.multiply(model_history.history['val_precision'],model_history.history['val_recall']), np.add(model_history.history['val_precision'], model_history.history['val_recall']))\n",
    "FP = model_history.history['false_positives']\n",
    "TP = model_history.history['true_positives']\n",
    "FN = model_history.history['false_negatives']\n",
    "TN = model_history.history['true_negatives']\n",
    "MCC = np.subtract(np.multiply(TP,TN),np.multiply(FP,FN))/np.sqrt(np.multiply(np.multiply(np.add(TP,FP),np.add(TP,FN)), np.multiply(np.add(TN,FP),np.add(TN,FN))))\n",
    "FP = model_history.history['val_false_positives']\n",
    "TP = model_history.history['val_true_positives']\n",
    "FN = model_history.history['val_false_negatives']\n",
    "TN = model_history.history['val_true_negatives']\n",
    "MCC_val = np.subtract(np.multiply(TP,TN),np.multiply(FP,FN))/np.sqrt(np.multiply(np.multiply(np.add(TP,FP),np.add(TP,FN)), np.multiply(np.add(TN,FP),np.add(TN,FN))))\n",
    "\n",
    "axs[0,1].plot(F1)\n",
    "axs[0,1].plot(F1_val)\n",
    "axs[0,1].set_title('model F1 score')\n",
    "axs[0,1].set_ylabel('F1')\n",
    "axs[0,1].set_xlabel('epoch')\n",
    "\n",
    "axs[1,1].plot(MCC)\n",
    "axs[1,1].plot(MCC_val)\n",
    "axs[1,1].set_title('model MCC')\n",
    "axs[1,1].set_ylabel('MCC')\n",
    "axs[1,1].set_xlabel('epoch')\n",
    "\n",
    "\n",
    "i = 1\n",
    "while os.path.exists(Models_path+\"Model%s.png\" % i):\n",
    "    i += 1\n",
    "plt.savefig(Models_path+\"Model%s.png\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a7844-4741-4845-98b4-556e730899df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883bd4b-b30e-40e4-a416-21cfd7fad22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    #print(pred_mask.shape)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]\n",
    "\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    \"\"\"\n",
    "    Displays the first image of each of the num batches\n",
    "    \"\"\"\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = unet.predict(image[tf.newaxis,:,:,:])\n",
    "            print(mask)\n",
    "            test=  np.squeeze(pred_mask)\n",
    "            print(test.shape)\n",
    "            #display([image, mask, test])\n",
    "            display([image, mask, create_mask(pred_mask)])\n",
    "    else:\n",
    "        display([sample_image, sample_mask,\n",
    "             create_mask(unet.predict(sample_image[tf.newaxis, ...]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a49a1-eab7-4c90-bb6c-b69eba857245",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdataset = get_dataset_large(tfr_dir = TFrecord_path, pattern = '*.tfrecords')\n",
    "print(testdataset)\n",
    "show_predictions(testdataset, 10)\n",
    "\n",
    "#####################change display to show 11microns -12 microns or whatever it should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c91ac-32a9-4539-b3d1-0503a4cb985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_mask = unet.predict(image[tf.newaxis,:,:,:])\n",
    "plt.imshow(\n",
    "    pr_mask[0]\n",
    ")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bdcdac-28a9-448e-b81e-5902515ac246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
