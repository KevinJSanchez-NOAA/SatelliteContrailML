{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ea7090-2a21-46f5-ae21-e3e50b2f77f2",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## Import Packages and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb437a0-ae1e-4c58-83f8-4f78626be60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('cpu_compiler', '/home/builder/ktietz/aggregate/tensorflow_recipes/ci_cpu/tensorflow-base_1614583966145/_build_env/bin/x86_64-conda_cos6-linux-gnu-gcc'), ('cuda_compute_capabilities', ['compute_35', 'compute_52', 'compute_60', 'compute_61', 'compute_70', 'compute_75']), ('cuda_version', '10.1'), ('cudnn_version', '7'), ('is_cuda_build', True), ('is_rocm_build', False)])\n",
      "cuda version:  10.1\n",
      "cudNN version:  7\n",
      "TF version:  2.4.1\n",
      "40\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "### -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May 24 13:26:13 2021\n",
    "\n",
    "@author: kjsanche\n",
    "\n",
    "Description: \n",
    "A function to load the 5 minute granules from MODIS channel 1 \n",
    "(0.65 microns) and the contrail mask for ML with a CNN.\n",
    "\n",
    "To do:\n",
    "\n",
    "-organize/markdown/comment code\n",
    "\n",
    "\n",
    "Input:\n",
    "Path   (string)\n",
    " \n",
    "        \n",
    "        \n",
    "Output:\n",
    "MODISCh1 (2D uint32)\n",
    "MASK     (2D uint16)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "from itertools import compress\n",
    "import numpy as np\n",
    "import struct\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from twilio.rest import Client\n",
    "from decouple import config\n",
    "from UNET_Functions import unet_model, summary\n",
    "from Sat_contrail_read import Extract_RawDef, extract_img, extract_mask, extract_imglist, get_model_memory_usage\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('/home/kjsanche/Desktop/Projects/loss')\n",
    "from loss_function import *\n",
    "from tensorflow.python.ops.metrics_impl import false_positives, false_negatives\n",
    "import tensorflow.keras.metrics as tfm\n",
    "import tensorflow_addons as tfa\n",
    "from focal_loss import BinaryFocalLoss, SparseCategoricalFocalLoss\n",
    "\n",
    "\n",
    "#config = tf.config.experimental\n",
    "#config.gpu_options.allow_growth = True\n",
    "#sess = tf.Session(config=config)\n",
    "\n",
    "sys_details = tf.sysconfig.get_build_info()\n",
    "print(sys_details)\n",
    "cudnn_version = sys_details[\"cudnn_version\"]\n",
    "cuda_version = sys_details[\"cuda_version\"]\n",
    "\n",
    "print('cuda version: ', cuda_version)\n",
    "print('cudNN version: ',cudnn_version)\n",
    "print('TF version: ', tf.version.VERSION)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "VALIDATION_SPLIT = .2\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 500\n",
    "\n",
    "IMG_W=64\n",
    "IMG_H=64\n",
    "N_CHANNELS = 3\n",
    "N_FILTERS = 64\n",
    "LEARNING_RATE = 0.00003\n",
    "AUTO = tf.data.AUTOTUNE # used in tf.data.Dataset API\n",
    "VERSION = '08'\n",
    "TFrecord_path ='/home/kjsanche/Desktop/ExternalSSD/SatContrailData/TFrecords/'\n",
    "Models_path ='/home/kjsanche/Desktop/ExternalSSD/SatContrailData/Models/'\n",
    "\n",
    "training_filenames=sorted(tf.io.gfile.glob([TFrecord_path + '*v' + VERSION + '.tfrecords']))\n",
    "\n",
    "if N_CHANNELS == 3:\n",
    "    validation_filenames=sorted(tf.io.gfile.glob([TFrecord_path + '*v3channelVALIDATION.tfrecords']))\n",
    "elif N_CHANNELS == 7:\n",
    "    validation_filenames=sorted(tf.io.gfile.glob([TFrecord_path + '*vVALIDATION.tfrecords']))\n",
    "\n",
    "\n",
    "random.Random(5).shuffle(training_filenames)\n",
    "random.Random(5).shuffle(validation_filenames)\n",
    "\n",
    "print(len(training_filenames))\n",
    "print(len(validation_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b91c4eb-b3d2-4e4c-ab7b-8ade63ec1299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "account_sid = config('ACCOUNT_ID',default='')\n",
    "auth_token = config('AUTHENTICATION_TOKEN',default='')\n",
    "phone_num = config('PHONE_NUMBER',default='')\n",
    "client = Client(account_sid, auth_token)\n",
    "\n",
    "#message = client.messages .create(\n",
    "#                    body =  \"Testing 1, 2, 3, 4\", #Message you send\n",
    "#                    from_ = '+12184383951',#Provided phone number\n",
    "#                    to =    phone_num)#Your phone number\n",
    "#message.sid\n",
    "\n",
    "class SMSCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch%50 == 0:\n",
    "            FP = logs['val_false_positives']\n",
    "            TP = logs['val_true_positives']\n",
    "            FN = logs['val_false_negatives']\n",
    "            TN = logs['val_true_negatives']\n",
    "            IOU_val = TP/(FN+TP+FP)\n",
    "            message = client.messages .create(\n",
    "                \n",
    "                body = \"For Epoch:{}, \"\n",
    "                        \"Training loss={:.2f}, Validation loss ={:.2f} and Validation IOU = {:.2f}\"\n",
    "                .format(epoch, logs[\"loss\"], logs[\"val_loss\"], IOU_val),\n",
    "\n",
    "                from_ = \"+12184383951\", to = phone_num)\n",
    "\n",
    "            print(message.sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73030cfe-a7cf-4833-bf27-d5fe9926be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfr_element(element):\n",
    "    \n",
    "    data = {\n",
    "      'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'width':tf.io.FixedLenFeature([], tf.int64),\n",
    "      'depth':tf.io.FixedLenFeature([], tf.int64),\n",
    "      'raw_label':tf.io.FixedLenFeature([], tf.string),#tf.string = bytestring (not text string)\n",
    "      'raw_image' : tf.io.FixedLenFeature([], tf.string),#tf.string = bytestring (not text string)\n",
    "    }\n",
    "\n",
    "\n",
    "    content = tf.io.parse_single_example(element, data)\n",
    "\n",
    "    height = content['height']\n",
    "    width = content['width']\n",
    "    depth = content['depth']\n",
    "    raw_label = content['raw_label']\n",
    "    raw_image = content['raw_image']\n",
    "\n",
    "\n",
    "    #get our 'feature'-- our image -- and reshape it appropriately\n",
    "    feature = tf.io.parse_tensor(raw_image, out_type=tf.float16)\n",
    "    feature = tf.reshape(feature, shape=[height,width,depth])\n",
    "    label = tf.io.parse_tensor(raw_label, out_type=tf.int8)\n",
    "    label = tf.reshape(label, shape=[height,width])\n",
    "    return (feature, label)\n",
    "\n",
    "def get_batched_dataset(filenames, testing = False):\n",
    "    option_no_order = tf.data.Options()\n",
    "    if testing:\n",
    "        option_no_order.experimental_deterministic = True\n",
    "    else:\n",
    "        option_no_order.experimental_deterministic = False\n",
    "\n",
    "    #dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.with_options(option_no_order)\n",
    "    dataset = dataset.interleave(tf.data.TFRecordDataset, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(parse_tfr_element, num_parallel_calls=AUTO)\n",
    "\n",
    "    dataset = dataset.cache() # If dataset fits in RAM\n",
    "    if not testing:\n",
    "        dataset = dataset.shuffle(2048)\n",
    "    #dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) \n",
    "    dataset = dataset.prefetch(AUTO) #\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_training_dataset(training_filenames):\n",
    "    return get_batched_dataset(training_filenames)\n",
    "\n",
    "def get_validation_dataset(validation_filenames):\n",
    "    return get_batched_dataset(validation_filenames)\n",
    "\n",
    "def get_test_dataset(test_filenames):\n",
    "    return get_batched_dataset(test_filenames, testing = True)\n",
    "\n",
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        #print(i)\n",
    "        #print(display_list[i].shape)\n",
    "        if i == 0:\n",
    "            plt.imshow(np.float32(display_list[i][:,:,7]))#-display_list[i][:,:,1]))\n",
    "        else:\n",
    "            plt.imshow(np.float32(1*display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95823e8e-cea2-4843-a8a1-88d23a1950da",
   "metadata": {},
   "source": [
    "The below code cell uses a lot of memory and therefore should only be used for testing and not be used during training."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f05f075-6b1b-4801-8e8d-e84c6e363970",
   "metadata": {},
   "source": [
    "testdataset = get_dataset_large(tfr_dir = TFrecord_path, pattern = '*.tfrecords')\n",
    "for image, mask in testdataset.take(3):\n",
    "    sample_image, sample_mask = image, mask\n",
    "    display([sample_image, sample_mask])\n",
    "print('mem usage: ', tf.config.experimental.get_memory_usage(\"GPU:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e431f70-397c-4ca4-8f8b-6461838d0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FocalTverskyLoss(targets, inputs, alpha=0.5, beta=0.5, gamma=1, smooth=1e-6):\n",
    "        '''\n",
    "        ... in the case of α=β=0.5 the Tversky index simplifies to be \n",
    "        the same as the Dice coefficient, which is also equal to the F1 \n",
    "        score. With α=β=1, Equation 2 produces Tanimoto coefficient, and \n",
    "        setting α+β=1 produces the set of Fβ scores. Larger βs weigh \n",
    "        recall higher than precision (by placing more emphasis on false negatives).\n",
    "        '''\n",
    "        targets = tf.cast(targets,tf.float32)\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = K.flatten(inputs)\n",
    "        targets = K.flatten(targets)\n",
    "        \n",
    "        #True Positives, False Positives & False Negatives\n",
    "        TP = K.sum((inputs * targets))\n",
    "        FP = K.sum(((1-targets) * inputs))\n",
    "        FN = K.sum((targets * (1-inputs)))\n",
    "               \n",
    "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth) \n",
    "\n",
    "        softdice = 2*TP/(K.sum(inputs**2)+K.sum(targets**2)+smooth)\n",
    "        Tversky = softdice\n",
    "        \n",
    "        FocalTversky = K.pow((1 - Tversky), gamma)\n",
    "\n",
    "        \n",
    "        \n",
    "        return FocalTversky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0e028-13ce-430a-929b-e189eab9259b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d158a868-eee7-4ead-a31e-399a439eca29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________________________________________\n",
      "Layer (type)                              Output Shape                 Param #         Connected to                               \n",
      "==================================================================================================================================\n",
      "input_1 (InputLayer)                      [(None, 64, 64, 3)]          0                                                          \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                           (None, 64, 64, 64)           1792            input_1[0][0]                              \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)                   (None, 64, 64, 64)           0               conv2d[0][0]                               \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                         (None, 64, 64, 64)           36928           leaky_re_lu[0][0]                          \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)                 (None, 64, 64, 64)           0               conv2d_1[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)              (None, 32, 32, 64)           0               leaky_re_lu_1[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                         (None, 32, 32, 128)          73856           max_pooling2d[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)                 (None, 32, 32, 128)          0               conv2d_2[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                         (None, 32, 32, 128)          147584          leaky_re_lu_2[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)                 (None, 32, 32, 128)          0               conv2d_3[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)            (None, 16, 16, 128)          0               leaky_re_lu_3[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                         (None, 16, 16, 256)          295168          max_pooling2d_1[0][0]                      \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)                 (None, 16, 16, 256)          0               conv2d_4[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                         (None, 16, 16, 256)          590080          leaky_re_lu_4[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)                 (None, 16, 16, 256)          0               conv2d_5[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)            (None, 8, 8, 256)            0               leaky_re_lu_5[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                         (None, 8, 8, 512)            1180160         max_pooling2d_2[0][0]                      \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)                 (None, 8, 8, 512)            0               conv2d_6[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                         (None, 8, 8, 512)            2359808         leaky_re_lu_6[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)                 (None, 8, 8, 512)            0               conv2d_7[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspose)        (None, 16, 16, 256)          1179904         leaky_re_lu_7[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "concatenate (Concatenate)                 (None, 16, 16, 512)          0               conv2d_transpose[0][0]                     \n",
      "                                                                                       leaky_re_lu_5[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                         (None, 16, 16, 256)          1179904         concatenate[0][0]                          \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)                 (None, 16, 16, 256)          0               conv2d_8[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                         (None, 16, 16, 256)          590080          leaky_re_lu_8[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)                 (None, 16, 16, 256)          0               conv2d_9[0][0]                             \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTranspose)      (None, 32, 32, 128)          295040          leaky_re_lu_9[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)               (None, 32, 32, 256)          0               conv2d_transpose_1[0][0]                   \n",
      "                                                                                       leaky_re_lu_3[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)                        (None, 32, 32, 128)          295040          concatenate_1[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)                (None, 32, 32, 128)          0               conv2d_10[0][0]                            \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)                        (None, 32, 32, 128)          147584          leaky_re_lu_10[0][0]                       \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)                (None, 32, 32, 128)          0               conv2d_11[0][0]                            \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTranspose)      (None, 64, 64, 64)           73792           leaky_re_lu_11[0][0]                       \n",
      "__________________________________________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)               (None, 64, 64, 128)          0               conv2d_transpose_2[0][0]                   \n",
      "                                                                                       leaky_re_lu_1[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)                        (None, 64, 64, 64)           73792           concatenate_2[0][0]                        \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)                (None, 64, 64, 64)           0               conv2d_12[0][0]                            \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)                        (None, 64, 64, 64)           36928           leaky_re_lu_12[0][0]                       \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)                (None, 64, 64, 64)           0               conv2d_13[0][0]                            \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)                        (None, 64, 64, 64)           36928           leaky_re_lu_13[0][0]                       \n",
      "__________________________________________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)                (None, 64, 64, 64)           0               conv2d_14[0][0]                            \n",
      "__________________________________________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)                        (None, 64, 64, 1)            65              leaky_re_lu_14[0][0]                       \n",
      "__________________________________________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze (TFOpLambda)         (None, 64, 64)               0               conv2d_15[0][0]                            \n",
      "__________________________________________________________________________________________________________________________________\n",
      "tf.math.sigmoid (TFOpLambda)              (None, 64, 64)               0               tf.compat.v1.squeeze[0][0]                 \n",
      "==================================================================================================================================\n",
      "Total params: 8,594,433\n",
      "Trainable params: 8,594,433\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________________________________________\n",
      "mem usage:  34386432\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#gamma>1 reduces the relative loss for well-classified examples \n",
    "#alpha is a weighted term whose value is α for positive(foreground) alpha = 1 does nothing. alpha = 0.25 is best\n",
    "#class and 1-α for negative(background) class.\n",
    "\n",
    "unet = unet_model((IMG_W, IMG_H, N_CHANNELS),n_filters=N_FILTERS,n_classes=1)\n",
    "#loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#loss=tfa.losses.SigmoidFocalCrossEntropy(),\n",
    "#loss=[BinaryFocalLoss(gamma=2,from_logits=True)],\n",
    "\n",
    "#Larger βs weigh recall higher than precision (by placing more emphasis on false negatives)\n",
    "#loss=FocalTverskyLoss(targets, inputs, alpha=0.5, beta=0.5, gamma=1, smooth=1e-6)\n",
    "unet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "              loss=[FocalTverskyLoss],\n",
    "              metrics=[tfm.Precision(), tfm.Recall(), tfm.FalseNegatives(), tfm.FalsePositives(), tfm.TruePositives(), tfm.TrueNegatives()])\n",
    "\n",
    "unet.summary(line_length = 130)\n",
    "print('mem usage: ', tf.config.experimental.get_memory_usage(\"GPU:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b76f90-e07b-4920-917c-2f4e1dbe8a22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.425\n",
      "WARNING:tensorflow:AutoGraph could not transform <function parse_tfr_element at 0x7f11aca9a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function parse_tfr_element at 0x7f11aca9a940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "<PrefetchDataset shapes: ((64, None, None, None), (64, None, None)), types: (tf.float16, tf.int8)>\n",
      "Epoch 1/500\n",
      "378/378 [==============================] - 239s 535ms/step - loss: 0.8600 - precision: 0.0099 - recall: 0.0056 - false_negatives: 814620.5488 - false_positives: 154404.0712 - true_positives: 1566.7916 - true_negatives: 48836072.6781 - val_loss: 0.9964 - val_precision: 8.7203e-04 - val_recall: 2.1559e-04 - val_false_negatives: 18550.0000 - val_false_positives: 4583.0000 - val_true_positives: 4.0000 - val_true_negatives: 167749088.0000\n",
      "SM8918c9a93a2b4fb3970056bcb7217ecf\n",
      "Epoch 2/500\n",
      "378/378 [==============================] - 202s 535ms/step - loss: 0.7941 - precision: 0.1400 - recall: 0.0798 - false_negatives: 730594.9499 - false_positives: 436931.2876 - true_positives: 84624.6174 - true_negatives: 48554502.6412 - val_loss: 0.9940 - val_precision: 0.0025 - val_recall: 0.0475 - val_false_negatives: 17672.0000 - val_false_positives: 351754.0000 - val_true_positives: 882.0000 - val_true_negatives: 167401872.0000\n",
      "Epoch 3/500\n",
      "378/378 [==============================] - 202s 536ms/step - loss: 0.7091 - precision: 0.2430 - recall: 0.2132 - false_negatives: 625233.5673 - false_positives: 552814.9604 - true_positives: 187799.7388 - true_negatives: 48440820.1372 - val_loss: 0.9914 - val_precision: 0.0045 - val_recall: 0.1219 - val_false_negatives: 16293.0000 - val_false_positives: 496647.0000 - val_true_positives: 2261.0000 - val_true_negatives: 167256928.0000\n",
      "Epoch 4/500\n",
      "378/378 [==============================] - 202s 536ms/step - loss: 0.6528 - precision: 0.3073 - recall: 0.2761 - false_negatives: 578698.0712 - false_positives: 508554.5673 - true_positives: 233552.6966 - true_negatives: 48485859.6728 - val_loss: 0.9897 - val_precision: 0.0057 - val_recall: 0.1124 - val_false_negatives: 16468.0000 - val_false_positives: 365709.0000 - val_true_positives: 2086.0000 - val_true_negatives: 167387808.0000\n",
      "Epoch 5/500\n",
      "378/378 [==============================] - 202s 536ms/step - loss: 0.6055 - precision: 0.3620 - recall: 0.3200 - false_negatives: 543607.8470 - false_positives: 458758.8047 - true_positives: 271584.8865 - true_negatives: 48532710.3931 - val_loss: 0.9868 - val_precision: 0.0070 - val_recall: 0.1463 - val_false_negatives: 15839.0000 - val_false_positives: 382510.0000 - val_true_positives: 2715.0000 - val_true_negatives: 167371264.0000\n",
      "Epoch 6/500\n",
      "378/378 [==============================] - 202s 536ms/step - loss: 0.5548 - precision: 0.4310 - recall: 0.3680 - false_negatives: 509899.0818 - false_positives: 393098.7784 - true_positives: 306198.1055 - true_negatives: 48597483.6121 - val_loss: 0.9866 - val_precision: 0.0081 - val_recall: 0.1850 - val_false_negatives: 15122.0000 - val_false_positives: 418112.0000 - val_true_positives: 3432.0000 - val_true_negatives: 167335616.0000\n",
      "Epoch 7/500\n",
      "378/378 [==============================] - 202s 536ms/step - loss: 0.5151 - precision: 0.4840 - recall: 0.4080 - false_negatives: 473558.6069 - false_positives: 346779.8549 - true_positives: 342332.1214 - true_negatives: 48643984.8918 - val_loss: 0.9843 - val_precision: 0.0096 - val_recall: 0.1495 - val_false_negatives: 15780.0000 - val_false_positives: 287193.0000 - val_true_positives: 2774.0000 - val_true_negatives: 167466480.0000\n",
      "Epoch 8/500\n",
      "378/378 [==============================] - 202s 536ms/step - loss: 0.4842 - precision: 0.5211 - recall: 0.4424 - false_negatives: 444135.2559 - false_positives: 317797.9604 - true_positives: 372091.8100 - true_negatives: 48672643.1662 - val_loss: 0.9802 - val_precision: 0.0118 - val_recall: 0.1943 - val_false_negatives: 14949.0000 - val_false_positives: 302841.0000 - val_true_positives: 3605.0000 - val_true_negatives: 167450832.0000\n",
      "Epoch 9/500\n",
      "378/378 [==============================] - 202s 536ms/step - loss: 0.4508 - precision: 0.5553 - recall: 0.4785 - false_negatives: 413626.4776 - false_positives: 295152.8179 - true_positives: 400910.2929 - true_negatives: 48696976.5831 - val_loss: 0.9796 - val_precision: 0.0131 - val_recall: 0.1834 - val_false_negatives: 15152.0000 - val_false_positives: 256731.0000 - val_true_positives: 3402.0000 - val_true_negatives: 167496960.0000\n",
      "Epoch 10/500\n",
      "378/378 [==============================] - 202s 536ms/step - loss: 0.4210 - precision: 0.5871 - recall: 0.5099 - false_negatives: 390059.8760 - false_positives: 279724.5515 - true_positives: 426240.3509 - true_negatives: 48710620.7810 - val_loss: 0.9794 - val_precision: 0.0133 - val_recall: 0.1823 - val_false_negatives: 15172.0000 - val_false_positives: 250227.0000 - val_true_positives: 3382.0000 - val_true_negatives: 167503424.0000\n",
      "Epoch 11/500\n",
      "378/378 [==============================] - 202s 535ms/step - loss: 0.4050 - precision: 0.6063 - recall: 0.5292 - false_negatives: 371631.6385 - false_positives: 266121.7414 - true_positives: 442309.6966 - true_negatives: 48726618.8047 - val_loss: 0.9799 - val_precision: 0.0130 - val_recall: 0.2056 - val_false_negatives: 14740.0000 - val_false_positives: 289284.0000 - val_true_positives: 3814.0000 - val_true_negatives: 167464288.0000\n",
      "Epoch 12/500\n",
      "327/378 [========================>.....] - ETA: 16s - loss: 0.3752 - precision: 0.6422 - recall: 0.5554 - false_negatives: 301668.2630 - false_positives: 210610.2936 - true_positives: 394054.1162 - true_negatives: 42085292.8777"
     ]
    }
   ],
   "source": [
    "print(get_model_memory_usage(BATCH_SIZE, unet))\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta = 0.005, patience=20, mode =\"min\", verbose = 2, restore_best_weights=True)\n",
    "training_data = get_training_dataset(training_filenames)\n",
    "validation_data = get_validation_dataset(validation_filenames)\n",
    "#test_data = get_test_dataset(test_filenames)\n",
    "print(training_data)\n",
    "model_history = unet.fit(training_data, validation_data=validation_data, epochs=EPOCHS, callbacks = [SMSCallback(), callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15ba292-6f6a-4b1e-965d-6471f2060feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9700c4f1-f666-42d5-b149-c27379da56d8",
   "metadata": {},
   "source": [
    "## unet.save_weights(Models_path+'model')\n",
    "#unet = tf.keras.models.load_model(Models_path+'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dbf810-7398-46ca-914a-1c3537bb5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(15,15))\n",
    "\n",
    "axs[0,0].plot(model_history.history['loss'])\n",
    "axs[0,0].plot(model_history.history['val_loss'])\n",
    "axs[0,0].set_title('model loss')\n",
    "axs[0,0].set_ylabel('loss')\n",
    "\n",
    "#axs[0].legend(['train', 'val'], loc='upper left')\n",
    "#axs[0].show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "F1 = 2*np.divide(np.multiply(model_history.history['precision'],model_history.history['recall']), np.add(model_history.history['precision'], model_history.history['recall']))\n",
    "F1_val = 2*np.divide(np.multiply(model_history.history['val_precision'],model_history.history['val_recall']), np.add(model_history.history['val_precision'], model_history.history['val_recall']))\n",
    "FP = model_history.history['false_positives']\n",
    "TP = model_history.history['true_positives']\n",
    "FN = model_history.history['false_negatives']\n",
    "TN = model_history.history['true_negatives']\n",
    "IOU = np.divide(TP,np.add(FN,np.add(TP,FP)))\n",
    "MCC = np.subtract(np.multiply(TP,TN),np.multiply(FP,FN))/np.sqrt(np.multiply(np.multiply(np.add(TP,FP),np.add(TP,FN)), np.multiply(np.add(TN,FP),np.add(TN,FN))))\n",
    "FP = model_history.history['val_false_positives']\n",
    "TP = model_history.history['val_true_positives']\n",
    "FN = model_history.history['val_false_negatives']\n",
    "TN = model_history.history['val_true_negatives']\n",
    "MCC_val = np.subtract(np.multiply(TP,TN),np.multiply(FP,FN))/np.sqrt(np.multiply(np.multiply(np.add(TP,FP),np.add(TP,FN)), np.multiply(np.add(TN,FP),np.add(TN,FN))))\n",
    "IOU_val = np.divide(TP,np.add(FN,np.add(TP,FP)))\n",
    "\n",
    "axs[1,0].plot(IOU)\n",
    "axs[1,0].plot(IOU_val)\n",
    "axs[1,0].set_title(f'model IoU, max = {np.max(IOU_val)}')\n",
    "axs[1,0].set_ylabel('IoU')\n",
    "axs[1,0].legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "axs[0,1].plot(F1)\n",
    "axs[0,1].plot(F1_val)\n",
    "axs[0,1].set_title('model F1 score')\n",
    "axs[0,1].set_ylabel('F1')\n",
    "axs[0,1].set_xlabel('epoch')\n",
    "\n",
    "axs[1,1].plot(MCC)\n",
    "axs[1,1].plot(MCC_val)\n",
    "axs[1,1].set_title('model MCC')\n",
    "axs[1,1].set_ylabel('MCC')\n",
    "axs[1,1].set_xlabel('epoch')\n",
    "\n",
    "\n",
    "i = 1\n",
    "while os.path.exists(Models_path+\"Model%s.png\" % i):\n",
    "    i += 1\n",
    "plt.savefig(Models_path+\"Model%s.png\" % i)\n",
    "\n",
    "# Save the weights \n",
    "unet.save_weights(Models_path+ \"Model%s.ckpt\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d53c4f-7bc4-489f-8967-46d672b669df",
   "metadata": {},
   "outputs": [],
   "source": [
    " message = client.messages .create(\n",
    "                \n",
    "    body = \"Model stopped at Epoch:{}, \"\n",
    "            \"max Validation IOU = {:.2f}\"\n",
    "    .format(len(IOU_val), np.max(IOU_val)),\n",
    "\n",
    "    from_ = \"+12184383951\", to = phone_num)\n",
    "\n",
    "print(message.sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee97d1-ba81-4916-9a53-4bf426274dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blockshaped(arr, nrows, ncols):\n",
    "    \"\"\"\n",
    "    Return an array of shape (n, nrows, ncols) where\n",
    "    n * nrows * ncols = arr.size\n",
    "\n",
    "    If arr is a 2D array, the returned array should look like n subblocks with\n",
    "    each subblock preserving the \"physical\" layout of arr.\n",
    "    \"\"\"\n",
    "    h, w = arr.shape\n",
    "    assert h % nrows == 0, f\"{h} rows is not evenly divisible by {nrows}\"\n",
    "    assert w % ncols == 0, f\"{w} cols is not evenly divisible by {ncols}\"\n",
    "    return (arr.reshape(h//nrows, nrows, -1, ncols)\n",
    "               .swapaxes(1,2)\n",
    "               .reshape(-1, nrows, ncols))\n",
    "def unblockshaped(arr, h, w):\n",
    "    \"\"\"\n",
    "    Return an array of shape (h, w) where\n",
    "    h * w = arr.size\n",
    "\n",
    "    If arr is of shape (n, nrows, ncols), n sublocks of shape (nrows, ncols),\n",
    "    then the returned array preserves the \"physical\" layout of the sublocks.\n",
    "    \"\"\"\n",
    "    n, nrows, ncols = arr.shape\n",
    "    return (arr.reshape(h//nrows, -1, nrows, ncols)\n",
    "               .swapaxes(1,2)\n",
    "               .reshape(h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501fda6-6b92-4f01-b5af-1204d5ddf223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_IOU(mask1, mask2):\n",
    "    inter = 0\n",
    "    union = 0\n",
    "    #for _, mask in image_ds:\n",
    "    #mask = mask.numpy()\n",
    "    mask1[mask1>1]=1\n",
    "    mask2[mask2>1]=1\n",
    "    mask1[mask1<1]=0\n",
    "    mask2[mask2<1]=0\n",
    "\n",
    "    #create list of true indicies for both masks\n",
    "    a = list(compress(range(len(mask1[:,:].flatten())), mask1[:,:].flatten()))\n",
    "    b = list(compress(range(len(mask2[:,:].flatten())), mask2[:,:].flatten()))\n",
    "\n",
    "    IOU = len(list(set(a).intersection(b)))/len(list(set(a).union(b)))\n",
    "    inter += len(list(set(a).intersection(b)))\n",
    "    union += len(list(set(a).union(b)))\n",
    "    #print(f\"intersection = {len(list(set(a).intersection(b)))}\")\n",
    "    #print(f\"union = {len(list(set(a).union(b)))}\")\n",
    "    print(f\"IOU ={IOU}\")\n",
    "    return inter, union, IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0685bf5-47a4-45e0-9196-52d4dd459a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def rescale_image(img):\n",
    "    #if NCHANNELS-5>0:\n",
    "    NCHANNELS = 7\n",
    "    tmp = img[:,:,:,NCHANNELS-5:]\n",
    "    tmp[tmp==17000]=0\n",
    "    img[:,:,:,NCHANNELS-5:]=tmp\n",
    "    BT_rng = np.float64([17000, 34000]) #this range is good for 4 upper channels. assuming range of 170-340 K (BT is multiplied by 100)\n",
    "    img[:,:,:,NCHANNELS-5:-1] = (img[:,:,:,NCHANNELS-5:-1] * ((BT_rng[1]-BT_rng[0])/2) ) + np.mean(BT_rng)\n",
    "    BT_rng = np.float64([0, 66535]) #lower 3 channels have a much larger range.\n",
    "    img[:,:,:,:NCHANNELS-5] = (img[:,:,:,:NCHANNELS-5] * ((BT_rng[1]-BT_rng[0])/2) ) + np.mean(BT_rng)\n",
    "    return img\n",
    "    #else:\n",
    "    #   img_rotated[img_rotated==0]=17000\n",
    "    #   BT_rng = np.float64([17000, 34000]) #this range is good for 4 upper channels. assuming range of 170-340 K (BT is multiplied by 100)\n",
    "    #   img_rotated[:,:,:,:-1] = (img_rotated[:,:,:,:-1]-np.mean(BT_rng)) / ((BT_rng[1]-BT_rng[0])/2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883bd4b-b30e-40e4-a416-21cfd7fad22b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    #print(pred_mask.shape)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]\n",
    "\n",
    "def display2(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'Old Mask', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        #print(i)\n",
    "        #print(display_list[i].shape)\n",
    "        if i == 0:\n",
    "            plt.imshow(np.float32(display_list[i][:,:,6]-display_list[i][:,:,5]), cmap = 'gray')\n",
    "        #elif i == 1:\n",
    "        #    plt.imshow(np.float32(display_list[i][:,:,0]-display_list[i][:,:,1]), cmap = 'gray')\n",
    "        elif i < 3:\n",
    "            plt.imshow(np.float32(1*display_list[i].numpy().squeeze()), cmap = 'gray')\n",
    "        else:\n",
    "            plt.imshow(np.float32(1*display_list[i].squeeze()), cmap = 'gray')\n",
    "        plt.axis('off')\n",
    "    _ = unet.evaluate(display_list[0][tf.newaxis,:,:,:], display_list[2][tf.newaxis,:,:], verbose=2)\n",
    "    #print(\"Untrained model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "    plt.show()\n",
    "\n",
    "def show_predictions(dataset=None,num=1):\n",
    "    \"\"\"\n",
    "    Displays the first image of each of the num batches\n",
    "    \"\"\"\n",
    "    if dataset:\n",
    "        for test_data, oldMask_data in dataset.take(num):\n",
    "            image, mask = test_data\n",
    "            image2, old_mask = oldMask_data\n",
    "            \n",
    "\n",
    "            _, _, _, FN, FP, TP, TN, _ = unet.evaluate(image.numpy(), mask.numpy(), verbose=2)\n",
    "            IOU = np.divide(TP,np.add(FN,np.add(TP,FP)))\n",
    "            print(f'IOU = {IOU}')\n",
    "            \n",
    "            #rescale image (scaled previously for training)\n",
    "            origImage = rescale_image(np.array(image,dtype=float))\n",
    "\n",
    "\n",
    "            #compile full image\n",
    "            fullimage = np.empty([2048, 4096, 7])\n",
    "            for i in range(origImage.shape[-1]):\n",
    "                fullimage[:,:,i] = unblockshaped(origImage[:,:,:,i], 2048, 4096)\n",
    "\n",
    "       \n",
    "\n",
    "            #compile both masks\n",
    "            fullOldMask = unblockshaped(old_mask.numpy(), 2048, 4096)\n",
    "            fullNewMask = unblockshaped(mask.numpy(), 2048, 4096)\n",
    "            \n",
    "            #plot image and masks\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(fullimage[:,:,4]-fullimage[:,:,5], cmap = 'gray')\n",
    "            plt.show()\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(np.float32(fullOldMask), cmap = 'gray')\n",
    "            plt.show()\n",
    "            Oldinter, Oldunion, OldIOU = image_IOU(np.float32(fullOldMask), np.float32(fullNewMask))\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(np.float32(fullNewMask), cmap = 'gray')\n",
    "            plt.show()\n",
    "\n",
    "            #run model for case to produced predicted mask and plot\n",
    "            pred_mask = unet.predict(image)\n",
    "            fullPredMask = unblockshaped(pred_mask, 2048, 4096)\n",
    "            Pinter, Punion, PIOU = image_IOU(np.float32(fullNewMask), fullPredMask)\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(np.float32(fullPredMask), cmap = 'gray')\n",
    "            plt.show()\n",
    "     \n",
    "            \n",
    "\n",
    "    else:\n",
    "        display([sample_image, sample_mask,\n",
    "             create_mask(unet.predict(sample_image[tf.newaxis, ...]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ca1cf-ef22-41b7-80da-e7a959e70920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU_test(dataset=None,num=1):\n",
    "    \"\"\"\n",
    "    extract IOU values for each image\n",
    "    \"\"\"\n",
    "\n",
    "    for test_data, oldMask_data in dataset.take(num):\n",
    "        image, mask = test_data\n",
    "        image2, old_mask = oldMask_data\n",
    "\n",
    "\n",
    "        #_, _, _, FN, FP, TP, TN, _ = unet.evaluate(image.numpy(), mask.numpy(), verbose=2)\n",
    "        #IOU = np.divide(TP,np.add(FN,np.add(TP,FP)))\n",
    "        #print(f'IOU = {IOU}')\n",
    "\n",
    "        #rescale image (scaled previously for training)\n",
    "        #origImage = rescale_image(np.array(image,dtype=float))\n",
    "\n",
    "\n",
    "        #compile full image\n",
    "        #fullimage = np.empty([2048, 4096, 7])\n",
    "        #for i in range(origImage.shape[-1]):\n",
    "        #    fullimage[:,:,i] = unblockshaped(origImage[:,:,:,i], 2048, 4096)\n",
    "\n",
    "\n",
    "\n",
    "        #compile both masks\n",
    "        fullOldMask = unblockshaped(old_mask.numpy(), 2048, 4096)\n",
    "        fullNewMask = unblockshaped(mask.numpy(), 2048, 4096)\n",
    "\n",
    "        #plot image and masks\n",
    "        #fig = plt.figure()\n",
    "        #plt.imshow(fullimage[:,:,4]-fullimage[:,:,5], cmap = 'gray')\n",
    "        #plt.show()\n",
    "        #fig = plt.figure()\n",
    "        #plt.imshow(np.float32(fullOldMask), cmap = 'gray')\n",
    "        #plt.show()\n",
    "        Oldinter, Oldunion, OldIOU = image_IOU(np.float32(fullOldMask), np.float32(fullNewMask))\n",
    "        #fig = plt.figure()\n",
    "        #plt.imshow(np.float32(fullNewMask), cmap = 'gray')\n",
    "        #plt.show()\n",
    "\n",
    "        #run model for case to produced predicted mask and plot\n",
    "        pred_mask = unet.predict(image)\n",
    "        fullPredMask = unblockshaped(pred_mask, 2048, 4096)\n",
    "        Pinter, Punion, PIOU = image_IOU(np.float32(fullNewMask), fullPredMask)\n",
    "        #fig = plt.figure()\n",
    "        #plt.imshow(np.float32(fullPredMask), cmap = 'gray')\n",
    "        #plt.show()\n",
    "    return Oldinter, Oldunion, OldIOU, Pinter, Punion, PIOU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f59a2a-ffcb-473e-9e9c-89589470cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from itertools import compress\n",
    "mpl.rcParams['figure.dpi']= 1000\n",
    "print(len(test_filenames))\n",
    "Oldinter = np.empty([len(test_filenames)])\n",
    "Oldunion = np.empty([len(test_filenames)])\n",
    "OldIOU = np.empty([len(test_filenames)])\n",
    "Pinter = np.empty([len(test_filenames)])\n",
    "Punion = np.empty([len(test_filenames)])\n",
    "PIOU = np.empty([len(test_filenames)])\n",
    "for cnt, (fname, fnameoldMask) in enumerate(zip(test_filenames,test_OldMASKfilenames)): \n",
    "    test_data = get_test_dataset(fname)\n",
    "    oldMask_data = get_test_dataset(fnameoldMask)\n",
    "    combined_dataset = tf.data.Dataset.zip((test_data,oldMask_data))\n",
    "    #show_predictions(combined_dataset, 1)\n",
    "    Oldinter[cnt], Oldunion[cnt], OldIOU[cnt], Pinter[cnt], Punion[cnt], PIOU[cnt] = IOU_test(combined_dataset, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
